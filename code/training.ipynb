{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO/u+m6L0tRnrmuWRExhysu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# EAI Final Project - Désirée Bellan\n","\n","Training code for the final project of the course Elective in Artificial Intelligence, held by prof. Christian Napoli and followed during the year 2020/2021"],"metadata":{"id":"zRAH2lRecAiO"}},{"cell_type":"markdown","source":["## Import Libraries"],"metadata":{"id":"L7n4fBclccST"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ieDx5O51b0H7"},"outputs":[],"source":["!pip install pytorch-lightning==2.0.1\n","!pip install torch_geometric\n","!pip install torch-cluster==1.6.1 -f https://data.pyg.org/whl/torch-2.0.0+cu118.html"]},{"cell_type":"code","source":["# import libraries\n","from torchvision import transforms as T\n","\n","from sklearn.metrics import confusion_matrix, classification_report\n","from typing import Callable, Optional, List, Union, Dict, Tuple\n","from torch_geometric.utils import to_dense_adj\n","from torchvision import datasets, models\n","from torch.utils.data import DataLoader\n","from torch_geometric.data import Data\n","from torch_cluster import knn_graph\n","from torch.optim import Adam\n","from google.colab import drive\n","\n","import torch_geometric.nn as gnn\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","import torch.nn as nn\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","\n","import torchmetrics\n","import torch_geometric\n","import PIL\n","import torch\n","import dlib\n","import math"],"metadata":{"id":"rR6szsPdcj6w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Path to Directories (Google Drive directories used during training)"],"metadata":{"id":"XuSfrTMGdUOa"}},{"cell_type":"code","source":["# directories\n","drive.mount('/content/drive', force_remount = True)\n","dir_='/content/drive/MyDrive/uni/EAI/'\n","data_path = dir_ + 'data/'\n","model_path = dir_ + 'model/'"],"metadata":{"id":"R6zgHlDBdZ-a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Metrics\n","\n","Metric utils to create plots and graphs with pytorch lightning"],"metadata":{"id":"ygWHnSokddz3"}},{"cell_type":"code","source":["def tocpu(list):\n","  return [elem.cpu().detach() for elem in list]\n","\n","class Metrics(pl.Callback):\n","  def __init__(self, name:str, plot = True):\n","\n","        self.name = name\n","        self.plot = plot\n","\n","        self.collection = {'train_loss': [], 'train_acc': [], 'train_f1': [], \n","                        'dev_loss': [], 'dev_acc': [], 'dev_f1': []}\n","        self.preds = torch.tensor([]).to(torch_device)\n","        self.targets = torch.tensor([]).to(torch_device)\n","\n","        self.val_epoch = {'loss': [], 'acc': [], 'f1': []}\n","\n","  def on_train_epoch_end(self, trainer, pl_module):\n","        self.collection['train_loss'].append(trainer.callback_metrics['train_loss'].tolist())\n","        self.collection['train_acc'].append(trainer.callback_metrics['train_acc'].tolist())\n","        self.collection['train_f1'].append(trainer.callback_metrics['train_f1'].tolist())\n","        \n","  def on_validation_epoch_end(self, trainer, pl_module):\n","        self.val_epoch['loss'].append(trainer.callback_metrics['valid_loss'].tolist())\n","        self.val_epoch['acc'].append(trainer.callback_metrics['valid_acc'].tolist())\n","        self.val_epoch['f1'].append(trainer.callback_metrics['valid_f1'].tolist())\n","    \n","  def on_validation_end(self, trainer, pl_module):\n","        self.collection['dev_loss'].append(sum(self.val_epoch['loss'])/len(self.val_epoch['loss']))\n","        self.collection['dev_acc'].append(sum(self.val_epoch['acc'])/len(self.val_epoch['acc']))\n","        self.collection['dev_f1'].append(sum(self.val_epoch['f1'])/len(self.val_epoch['f1']))\n","        self.val_epoch['loss'] = []\n","        self.val_epoch['acc'] = []\n","        self.val_epoch['f1'] = []\n","\n","  def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n","      self.preds = torch.cat([self.preds, outputs['preds']])\n","      self.targets = torch.cat([self.targets, outputs['targets']])\n","    \n","  def on_test_end(self, trainer, pl_module):\n","      num_classes = pl_module.hparams['output-channels']\n","      if self.plot:\n","        # PLOT CURVES\n","        fig, axs = plt.subplots(1,3, figsize = (9, 3))\n","        x = range(len(self.collection['train_loss']))\n","        axs[0].set_title('Loss')\n","        axs[0].plot(x, self.collection['train_loss'], label = 'train')\n","        axs[0].plot(x,self.collection['dev_loss'][1:len(self.collection['train_loss'])+1], label = 'valid')\n","        #axs[0].set_ylim(bottom = -200, top = 100)\n","        axs[1].set_title('Accuracy')\n","        axs[1].plot(x, self.collection['train_acc'], label = 'train')\n","        axs[1].plot(x, self.collection['dev_acc'][1:len(self.collection['train_loss'])+1], label = 'valid')\n","        axs[2].set_title('F1 Score')\n","        axs[2].plot(x, self.collection['train_f1'], label = 'train')\n","        axs[2].plot(x, self.collection['dev_f1'][1:len(self.collection['train_loss'])+1], label = 'valid')\n","\n","        handles, labels = [a for a in axs[0].get_legend_handles_labels()]\n","        fig.legend(handles, labels, loc='upper right')\n","\n","        plt.show()\n","        plt.savefig('{}_plot.png'.format(self.name))\n","\n","      # PLOT CONFUSION MATRIX\n","      task = 'multiclass'\n","      num_classes = 2 \n","      ConfusionMatrix = torchmetrics.ConfusionMatrix(task, num_classes=num_classes).to(torch_device)\n","      plt.figure(figsize = (8,8))\n","      cfm = ConfusionMatrix(self.preds.float(), self.targets).cpu().detach().numpy()\n","      for i in range(num_classes):\n","          cfm[:,i] = cfm[:,i]/np.sum(cfm[:,i])*100 if np.sum(cfm[:,i])!= 0 else cfm[:,i]\n","      class_names = ['no-syndrom', 'syndrom'] if num_classes == 2 else list(range(1, num_classes + 1))\n","      dataframe = pd.DataFrame(cfm, index=class_names, columns=class_names)\n","      # Create heatmap\n","      sns.heatmap(dataframe, annot = True, cmap=\"YlGnBu\", xticklabels=2) \n","      plt.title(\"Confusion Matrix\")\n","      \n","      plt.ylabel(\"True Class\"), \n","      plt.xlabel(\"Predicted Class\")\n","      plt.show()\n","      plt.savefig('{}_cfm.png'.format(self.name))\n","\n","      del dataframe\n","      del cfm\n","\n","      self.preds = (self.preds >= 0.5).int()\n","      print(classification_report(tocpu(self.targets), tocpu(self.preds)))\n","\n","  def on_save_checkpoint(self, trainer, pl_module, checkpoint) -> None:\n","        checkpoint['metrics'] = {'collection': self.collection}\n","        checkpoint['current_epoch'] = trainer.current_epoch\n","        \n","  def load(self, ckpt_path) -> None:\n","        checkpoint = torch.load(ckpt_path, map_location=torch_device) \n","        self.collection = checkpoint['metrics']['collection']\n","\n","torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpus = 1 if torch_device == torch.device('cuda') else  0"],"metadata":{"id":"iIXQT0uGdnci"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Datasets"],"metadata":{"id":"IogtzA9pd9XB"}},{"cell_type":"markdown","source":["### Landmark Identification Utils"],"metadata":{"id":"h0NYP5HUeyLD"}},{"cell_type":"code","source":["detector = dlib.get_frontal_face_detector()\n","predictor = dlib.shape_predictor(model_path + '/dlib/shape_predictor_68_face_landmarks.dat')"],"metadata":{"id":"D94sWGYNe0_w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Graph Dataset"],"metadata":{"id":"29UY-75Md_JW"}},{"cell_type":"code","source":["class faceGraphDataset(datasets.ImageFolder):\n","    def __init__(self, root):\n","      super().__init__(root)\n","      self.data = []\n","      for i, (image_path, target) in enumerate(self.samples):\n","        self.create_graph(image_path, target, i)\n","\n","    def create_graph(self, image_path:str, target:int, batch_idx:int):\n","      # Load face image\n","      img = dlib.load_rgb_image(image_path)\n","      # Detect face\n","      faces = detector(img, 1)\n","      if len(faces) > 0:\n","        # Detect facial landmarks\n","        landmarks = predictor(img, faces[0])\n","        landmarks = [[landmarks.part(i).x, landmarks.part(i).y] for i in range(68)]\n","        # Define nodes\n","        nodes = torch.tensor(landmarks, dtype=torch.float)\n","        batch = torch.tensor([batch_idx for _ in range(nodes.shape[0])])\n","        # Define edges\n","        edges = knn_graph(nodes, k=6, batch = batch) # connect each node to its 6 nearest neighbors\n","        # Define weights\n","        distances = torch.norm(x[edges[0]] - x[edges[1]], p=2, dim=1)\n","        # Assign features\n","        features = torch.zeros((68, 3, 32, 32)) # create a tensor to hold features\n","        for i, landmark in enumerate(landmarks):\n","            x, y = landmark\n","            x += -min(0, x-16) - max(max(img.shape[0], x+16) - img.shape[0],0)\n","            y += -min(0, y-16) - max(max(img.shape[1], y+16) - img.shape[1],0)\n","            feature = img[x-16:x+16, y-16:y+16] # crop a 32x32 region around the landmark point\n","            features[i] = torch.tensor(feature, dtype=torch.float).permute(2, 0, 1) # convert to tensor and permute dimensions to match PyTorch convention\n","        # Create adjacency matrix\n","        adj = to_dense_adj(edges)\n","        # Create PyTorch Geometric Data object\n","        self.data.append((Data(x=features, edge_index=edges, edge_attr=adj, edge_weights=distances), target))\n","\n","    def __getitem__(self, index):\n","      return self.data[index]\n","    def __len__(self):\n","      return len(self.data)"],"metadata":{"id":"djXjl5vVeAwl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### VGG Dataset"],"metadata":{"id":"unmkFTryeJaK"}},{"cell_type":"code","source":["class faceDataset(datasets.ImageFolder):\n","\n","  def shape_to_np(self, shape, dtype=\"int\"):\n","    # initialize the list of (x, y)-coordinates\n","    coords = np.zeros((68, 2), dtype=dtype)\n","    # loop over the 68 facial landmarks and convert them\n","    # to a 2-tuple of (x, y)-coordinates\n","    for i in range(0, 68):\n","      coords[i] = (shape.part(i).x, shape.part(i).y)\n","    # return the list of (x, y)-coordinates\n","    return coords\n","\n","  def find_features(self, path, img_to_crop):\n","    # find landmarks\n","    img = dlib.load_rgb_image(path)\n","    face = detector(img, 1)\n","    features = []\n","    if len(face)>0:\n","      face = face[0]\n","      shape = predictor(img, face)\n","      shape = self.shape_to_np(shape)\n","      if landmarks == 1:\n","        features = [img_to_crop.crop((face.left(), face.top(), face.right(), face.bottom()))]\n","      # case divided into three parts : eyes, nose, mouth\n","      if landmarks == 3:\n","        eyes = img_to_crop.crop((face.left(), face.top(), face.right(), shape[29][1]))\n","        nose = img_to_crop.crop((face.left(), shape[21][1], face.right(), shape[50][1]))\n","        mouth = img_to_crop.crop((face.left(), shape[33][1], face.right(), face.bottom()))\n","        features = [eyes, nose, mouth]\n","      # case divided into four parts : left eye, right eye, nose, mouth\n","      elif landmarks == 4:\n","        leye = img_to_crop.crop((shape[17][0],shape[20][1],shape[27][0],shape[29][1]))\n","        reye = img_to_crop.crop((shape[27][0],shape[24][1],shape[26][0],shape[29][1]))\n","        nose = img_to_crop.crop((shape[40][0],shape[27][1],shape[47][0],shape[51][1]))\n","        mouth = img_to_crop.crop((shape[41][0],shape[33][1],shape[46][0],shape[5][1]))\n","        features = [leye, reye, nose, mouth]\n","    else:\n","      w, h = img_to_crop.size\n","      if landmarks ==  1:\n","        features = [img_to_crop]\n","      if landmarks == 3:\n","        eyes = img_to_crop.crop((0, 0, w, h//3))\n","        nose = img_to_crop.crop((0, h//3, w, h*2//3))\n","        mouth = img_to_crop.crop((0, h*2//3, w, h))\n","        features = [eyes, nose, mouth]\n","      elif landmarks == 4:\n","        leye = img_to_crop.crop((0,0,w//2,h//3))\n","        reye = img_to_crop.crop((w//2,0,w,h//3))\n","        nose = img_to_crop.crop((0,h//3,w,2*h//3))\n","        mouth = img_to_crop.crop((0,2*h//3,w,h))\n","        features = [leye, reye, nose, mouth]\n","    return features\n","\n","  def __getitem__(self, index:int):\n","    path, target = self.samples[index]\n","    sample = self.loader(path)  \n","    # divide image into landmarks\n","    faceFeatures= self.find_features(path, sample)\n","    input = {'input' : sample}\n","    for i, feature in enumerate(faceFeatures):\n","      input[i] = feature\n","    for i in input:\n","      if self.transform is not None:\n","        input[i] = self.transform(input[i])\n","    if self.target_transform is not None:\n","      target = self.target_transform(target)\n","    return input, target"],"metadata":{"id":"U5-OYFnjeMW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataloaders"],"metadata":{"id":"hdbtHIjOeZMY"}},{"cell_type":"markdown","source":["### Graph Dataloader"],"metadata":{"id":"tqB-Mf2Yealj"}},{"cell_type":"code","source":["class graphDataloader(pl.LightningDataModule):\n","  def __init__(self, root:str, batch_size:int, gan:bool = False):\n","    super().__init__()\n","    self.root = root\n","    self.batch_size = batch_size\n","    self.train_path = root + 'train' if not gan else root + 'train_augmented'\n","    self.dev_path = root + 'dev' if not gan else root + 'dev_augmented'\n","    self.test_path = root + 'test' if not gan else root + 'test_augmented'\n","  def setup(self, stage: Optional[str] = None) -> None:\n","      if stage == \"fit\":\n","        self.train_dataset = faceGraphDataset(self.train_path)\n","        try: self.validation_dataset = faceGraphDataset(self.dev_path)\n","        except: self.validation_dataset = faceGraphDataset(self.test_path)\n","      if stage == 'validate':\n","        try : self.validation_dataset = faceGraphDataset(self.dev_path)\n","        except : self.validation_dataset = faceGraphDataset(self.test_path)\n","      if stage == 'test':\n","        self.test_dataset = faceGraphDataset(self.test_path)\n","  def train_dataloader(self, *args, **kwargs) -> DataLoader:\n","        return torch_geometric.loader.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n","\n","  def val_dataloader(self, *args, **kwargs) -> Union[DataLoader, List[DataLoader]]:\n","        return torch_geometric.loader.DataLoader(self.validation_dataset, batch_size=self.batch_size, shuffle=True)\n","\n","  def test_dataloader(self, *args, **kwargs) -> Union[DataLoader, List[DataLoader]]:\n","        return torch_geometric.loader.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)"],"metadata":{"id":"ekDGtbfsecYQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### VGG Dataloader"],"metadata":{"id":"RQrT44Bbecly"}},{"cell_type":"code","source":["class dataloader(pl.LightningDataModule):\n","  def __init__(self, root:str, transform:T, batch_size:int, augment:bool = True, gan:bool = False):\n","    super().__init__()\n","    self.batch_size = batch_size\n","    self.train_transform = self.test_transform = transform\n","    if augment : \n","      self.train_transform = T.Compose([\n","          T.RandomHorizontalFlip(), \n","          T.RandomAffine(degrees = (-5,5), translate = (0.05, 0.05), shear = 5*math.pi/180),\n","          T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n","          T.RandomEqualize(),\n","          T.GaussianBlur(3),\n","          self.train_transform,\n","          T.RandomErasing()])\n","    self.train_path = root + 'train' if not gan else root + 'train_augmented'\n","    self.dev_path = root + 'dev' if not gan else root + 'dev_augmented'\n","    self.test_path = root + 'test' if not gan else root + 'test_augmented'\n","  def setup(self, stage: Optional[str] = None) -> None:\n","      if stage == \"fit\":\n","        self.train_dataset = faceDataset(self.train_path, transform = self.train_transform)\n","        try: self.validation_dataset = faceDataset(self.dev_path, transform = self.test_transform)\n","        except: self.validation_dataset = faceDataset(self.test_path, transform = self.test_transform)\n","      if stage == 'validate':\n","        try : self.validation_dataset = faceDataset(self.dev_path, transform = self.test_transform)\n","        except : self.validation_dataset = faceDataset(self.test_path, transform = self.test_transform)\n","      if stage == 'test':\n","        self.test_dataset = faceDataset(self.test_path, transform = self.test_transform)\n","  def train_dataloader(self, *args, **kwargs) -> DataLoader:\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n","\n","  def val_dataloader(self, *args, **kwargs) -> Union[DataLoader, List[DataLoader]]:\n","        return DataLoader(self.validation_dataset, batch_size=self.batch_size, shuffle=True)\n","\n","  def test_dataloader(self, *args, **kwargs) -> Union[DataLoader, List[DataLoader]]:\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)"],"metadata":{"id":"WKpRbZDPenx1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## VGG Model"],"metadata":{"id":"tVzEkuK7fIaS"}},{"cell_type":"code","source":["from drive.MyDrive.uni.EAI.data.vgg_face import vgg_face_dag\n","\n","class classification(pl.LightningModule):\n","  def __init__(self, hparams):\n","    super(classification, self).__init__()\n","    self.hparams.update(hparams)\n","    self.hparams['VGG_FACE_WEIGHTS'] = data_path + 'vgg_face_dag.pth'\n","    self.save_hyperparameters()\n","    self.model = vgg_face_dag(self.hparams['VGG_FACE_WEIGHTS'])\n","    self.model.fc8 = nn.Linear(4096, self.hparams['output-channels'], bias = True)\n","    self.loss = nn.CrossEntropyLoss(weight = torch.tensor([self.hparams['weight'], 1.]))\n","    task = 'multiclass'\n","    self.val_f1 = torchmetrics.F1Score(task = task, num_classes = self.hparams['output-channels'], average = 'macro')\n","    self.accuracy = torchmetrics.Accuracy(task = task, num_classes = self.hparams['output-channels'], average = 'macro')\n","  def forward(self, x, last_layer = True):\n","    return F.softmax(self.model(x, last_layer), dim = -1)\n","\n","  def training_step(self, batch, batch_idx):\n","    inputs, labels = batch\n","    feature = inputs[self.hparams['feature']]\n","    logits = self.forward(feature)\n","    loss = self.loss(logits, labels)\n","    predictions = torch.argmax(logits, dim = -1).view(-1)\n","    labels = labels.view(-1)\n","    #print(prediction)\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n"," \n","    self.log('train_loss', loss.item(), prog_bar=True)\n","    self.log('train_f1', sample_f1*100)\n","    self.log('train_acc', sample_accuracy*100)\n","\n","    return loss\n","\n","  def validation_step(self, batch, batch_idx):\n","    inputs, labels = batch\n","    feature = inputs[self.hparams['feature']]\n","    logits = self.forward(feature)\n","    loss = self.loss(logits, labels)\n","    predictions = torch.argmax(logits, dim = -1).view(-1)\n","    labels = labels.view(-1)\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n","\n","    self.log('valid_f1', sample_f1*100, prog_bar=True)\n","    self.log('valid_acc', sample_accuracy*100, prog_bar=True)\n","    self.log('valid_loss', loss)\n","\n","  def test_step(self, batch, batch_idx):\n","    inputs, labels = tuple(batch)\n","    feature = inputs[self.hparams['feature']]\n","    logits = self.forward(feature)\n","    loss = self.loss(logits, labels)\n","    predictions = torch.argmax(logits, dim = -1).view(-1)\n","    labels = labels.view(-1)\n","\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n"," \n","    return {'preds': predictions, 'targets': labels}\n","\n","  def configure_optimizers(self):\n","    return Adam(self.parameters(), lr = self.hparams['lr'], weight_decay = 1e-2)"],"metadata":{"id":"-hXjM6EKfLJv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## N-Feature Combination Classifier"],"metadata":{"id":"OXFQJ-EPfeZe"}},{"cell_type":"code","source":["class ClassificationUsingFeatures(pl.LightningModule):\n","  def __init__(self, hparams:dict, features:List[str]):\n","    super(ClassificationUsingFeatures, self).__init__()\n","    self.hparams.update(hparams)\n","    self.save_hyperparameters()\n","    self.features = nn.ModuleList([classification.load_from_checkpoint(path) for path in features])\n","    for module in self.features:\n","      for param in module.parameters():\n","        param.requires_grad = False\n","    input_dim = 2*len(self.features)\n","    self.classifier = nn.Sequential(\n","        nn.Linear(input_dim,input_dim*8),\n","        nn.Dropout(self.hparams['dropout']),\n","        nn.ReLU(),\n","        nn.Linear(input_dim*8,input_dim*4),\n","        nn.Dropout(self.hparams['dropout']),\n","        nn.ReLU(),\n","        nn.Linear(input_dim*4,2))\n","    self.loss = nn.CrossEntropyLoss(weight = torch.tensor([self.hparams['weight'], 1.]))\n","    self.val_f1 = torchmetrics.F1Score(task = \"multiclass\", num_classes = 2, average = 'macro')\n","    self.accuracy = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 2, average = 'macro')\n","\n","  def forward(self, features):\n","    for i, module in enumerate(self.features):\n","      features[i] = module(features[i])\n","    x = torch.cat(features, dim=1)\n","    return F.softmax(self.classifier(x), dim = -1)\n","\n","  def training_step(self, batch, batch_idx):\n","    inputs, labels = batch\n","    features = [image for key, image in inputs.items()]\n","    logits = self.forward(features)\n","    loss = self.loss(logits, labels)\n","    predictions = torch.argmax(logits, dim = -1).view(-1)\n","    labels = labels.view(-1)\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n"," \n","    self.log('train_loss', loss.item(), prog_bar=True)\n","    self.log('train_f1', sample_f1*100)\n","    self.log('train_acc', sample_accuracy*100)\n","\n","    return loss\n","\n","  def validation_step(self, batch, batch_idx):\n","    inputs, labels = batch\n","    features = [image for key, image in inputs.items()]\n","    logits = self.forward(features)\n","    loss = self.loss(logits, labels)\n","    predictions = torch.argmax(logits, dim = -1).view(-1)\n","    labels = labels.view(-1)\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n","\n","    self.log('valid_f1', sample_f1*100, prog_bar=True)\n","    self.log('valid_acc', sample_accuracy*100, prog_bar=True)\n","    self.log('valid_loss', loss)\n","\n","  def test_step(self, batch, batch_idx):\n","    inputs, labels = batch\n","    features = [image for key, image in inputs.items()]\n","    logits = self.forward(features)\n","    predictions = torch.argmax(logits, dim = -1).view(-1)\n","    labels = labels.view(-1)\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n","\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n"," \n","    return {'preds': predictions, 'targets': labels}\n","\n","  def configure_optimizers(self):\n","    return Adam(self.parameters(), lr = self.hparams['lr'], weight_decay = 1e-2)"],"metadata":{"id":"EVYDIFXMfg1y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Graph CNNs Face Classification Model"],"metadata":{"id":"ij75juaafsBU"}},{"cell_type":"code","source":["class GCNNFaceRecognition(pl.LightningModule):\n","  def __init__(self, hparams, num_features=68 , hidden_channels = 128, num_classes = 2):\n","        super(GCNNFaceRecognition, self).__init__()\n","        self.hparams.update(hparams)\n","        self.save_hyperparameters()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size = 3, padding = 1, stride = 2),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size = 3, padding = 1, stride = 2),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size = 3, padding = 1, stride = 2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size = 3, padding = 1, stride = 2),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, kernel_size = 3, padding = 1, stride = 2),\n","            nn.ReLU()\n","        )\n","        self.graph_cnn1 = gnn.GCNConv(256, hidden_channels)\n","        self.graph_cnn2 = gnn.GCNConv(hidden_channels, 2)\n","        self.fc = nn.Linear(num_features, 1)\n","        self.loss = nn.CrossEntropyLoss(weight = torch.tensor([self.hparams['weight'], 1.]))\n","        self.val_f1 = torchmetrics.F1Score(task = \"multiclass\", num_classes = 2, average = 'macro')\n","        self.accuracy = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 2, average = 'macro')\n","\n","  def forward(self, data):\n","        num_graphs = data.num_graphs\n","        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weight\n","        num_features = x.shape[0]//num_graphs\n","        x = self.conv(x).squeeze()\n","        x = F.relu(self.graph_cnn1(x, edge_index, edge_weight))\n","        x = F.relu(self.graph_cnn2(x, edge_index, edge_weight))\n","        x = self.fc(x.reshape(num_graphs, num_features, 2).permute(0,2,1)).squeeze()\n","        return x\n","  def training_step(self, batch, batch_idx):\n","    input, labels = batch\n","    logits = self.forward(input)\n","    loss = self.loss(logits, labels)\n","    predictions = torch.argmax(logits, dim = -1).view(-1)\n","    labels = labels.view(-1)\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n"," \n","    self.log('train_loss', loss.item(), prog_bar=True)\n","    self.log('train_f1', sample_f1*100)\n","    self.log('train_acc', sample_accuracy*100)\n","\n","    return loss\n","\n","  def validation_step(self, batch, batch_idx):\n","    input, labels = batch\n","    logits = self.forward(input)\n","    loss = self.loss(logits, labels)\n","    predictions = torch.argmax(logits, dim = -1).view(-1)\n","    labels = labels.view(-1)\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n","\n","    self.log('valid_f1', sample_f1*100, prog_bar=True)\n","    self.log('valid_acc', sample_accuracy*100, prog_bar=True)\n","    self.log('valid_loss', loss)\n","\n","  def test_step(self, batch, batch_idx):\n","    input, labels = batch\n","    logits = self.forward(input)\n","    predictions = torch.argmax(logits, dim = -1).view(-1)\n","    labels = labels.view(-1)\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n","\n","    sample_f1 = self.val_f1(predictions, labels)\n","    sample_accuracy = self.accuracy(predictions, labels)\n"," \n","    return {'preds': predictions, 'targets': labels}\n","\n","  def configure_optimizers(self):\n","    return Adam(self.parameters(), lr = self.hparams['lr'], weight_decay = 1e-2)"],"metadata":{"id":"4Wg7fRqOfzKg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"JedwtBHVf5MZ"}},{"cell_type":"markdown","source":["#### Whole Face "],"metadata":{"id":"Zz1SXfOh5ixK"}},{"cell_type":"code","source":["# VGG16 Model with augmentation for syndrome/no-syndrome classification pretrained over vgg-face\n","landmarks = 1\n","hparams = {\n","    'augment' : False,\n","    'max_epochs' : 500,\n","    'batch_size': 32,\n","    'input-channels':3,\n","    'output-channels':2,\n","    'feature' : 0,\n","    'optimizer': 'Adam',\n","    'landmarks' : 1,\n","    'VGG_FACE_WEIGHTS':data_path + 'vgg_face_dag.pth',\n","    'vgg' : 'vgg-face',\n","    'lr': 2e-5,\n","    'weight': 0.2\n","    }\n","checkpoint_dir = model_path + 'VGG'\n","check_point_callback = pl.callbacks.ModelCheckpoint(\n","    monitor='valid_loss',  \n","    verbose=True, \n","    save_top_k = 2,  \n","    mode='min', \n","    dirpath= checkpoint_dir,  \n","    filename='{epoch}-{valid_f1:.4f}')\n","early_stopping = pl.callbacks.EarlyStopping(\n","    monitor='valid_loss',  \n","    patience = 10,  \n","    verbose=True,  \n","    mode='min',\n",")\n","model = classification(hparams)\n","model.to(torch_device)\n","model.train()\n","metric = Metrics(name = 'VGG')\n","transform = models.VGG16_Weights.IMAGENET1K_V1.transforms\n","data_module = dataloader(data_path, transform, hparams['batch_size'], augment = hparams['augment'], gan = hparams['gan'])\n","trainer = pl.Trainer(val_check_interval=1.0, max_epochs = hparams['max_epochs'], default_root_dir= checkpoint_dir,  callbacks = [check_point_callback, metric, early_stopping])\n","trainer.fit(model, datamodule = data_module)"],"metadata":{"id":"SfUPxNuqgHop"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, augmentation = True, gan = False, batch_size = 32\n","ckpt_path = model_path + 'best_checkpoints/best_whole-face_augment_95.1515.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"zZmPc8ih6wXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, augmentation = False, gan = True, batch_size = 32\n","ckpt_path = model_path + 'best_checkpoints/best_whole-face_weight=0.3_loss=0.337_noagumented_gan.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"ThJgEWiO8ugb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, augmentation = False, gan = False, batch_size = 32\n","ckpt_path = model_path + 'best_checkpoints/best_whole-face_noaugment(97.6698).ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"1uzbzN0l9ElE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.2, augmentation = True, gan = True, batch_size = 32\n","ckpt_path = model_path + 'best_checkpoints/best_whole-face_weight=0.2_loss=0.321_gan.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"fEos-yw49bM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GCNNs weights = 0.2, gan = False, batch_size = 16\n","ckpt_path = model_path + 'best_checkpoints/best_whole-face_graph.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"xompOdIu-LWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GCNNs weights = 0.2, gan = True, batch_size = 32\n","ckpt_path = model_path + 'best_checkpoints/best_whole-face_graph_gan.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"NfgKYfww_NvU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Eyes, Nose And Mouth Features"],"metadata":{"id":"8Et4fBiK_Vwd"}},{"cell_type":"markdown","source":["#### Eyes"],"metadata":{"id":"bMyRr0lZ_kG0"}},{"cell_type":"code","source":["# VGG16 Model with augmentation for syndrome/no-syndrome classification pretrained over vgg-face\n","landmarks = 3\n","hparams = {\n","    'augment' : True,\n","    'max_epochs' : 500,\n","    'batch_size': 32,\n","    'input-channels':3,\n","    'output-channels':2,\n","    'feature' : 0,\n","    'VGG_FACE_WEIGHTS':data_path + 'vgg_face_dag.pth',\n","    'vgg' : 'vgg-face',\n","    'lr': 2e-5,\n","    'weight': 0.2\n","    }\n","checkpoint_dir = model_path + 'VGG'\n","check_point_callback = pl.callbacks.ModelCheckpoint(\n","    monitor='valid_loss',  \n","    verbose=True, \n","    save_top_k = 2,  \n","    mode='min', \n","    dirpath= checkpoint_dir,  \n","    filename='{epoch}-{valid_f1:.4f}')\n","early_stopping = pl.callbacks.EarlyStopping(\n","    monitor='valid_loss',  \n","    patience = 10,  \n","    verbose=True,  \n","    mode='min',\n",")\n","model = classification(hparams)\n","model.to(torch_device)\n","model.train()\n","metric = Metrics(name = 'VGG_eyes')\n","transform = models.VGG16_Weights.IMAGENET1K_V1.transforms\n","data_module = dataloader(data_path, transform, hparams['batch_size'], augment = hparams['augment'], gan = hparams['gan'])\n","trainer = pl.Trainer(val_check_interval=1.0, max_epochs = hparams['max_epochs'], default_root_dir= checkpoint_dir,  callbacks = [check_point_callback, metric, early_stopping])"],"metadata":{"id":"kIDTn86T_fNg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, batch_size = 32, augment = True, gan = False\n","ckpt_path = model_path + 'best_checkpoints/best_eyes_weights=0.3_88.3491.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"0IV-tH31_yGY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, batch_size = 32, augment = False, gan = False\n","ckpt_path = model_path + 'best_checkpoints/best_eyes_weights=0.3_noaugment_96.1212.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"GB3QAmnUAtAj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Nose"],"metadata":{"id":"2SswJwKTBJTp"}},{"cell_type":"code","source":["# VGG16 Model with augmentation for syndrome/no-syndrome classification pretrained over vgg-face\n","landmarks = 3\n","hparams = {\n","    'augment' : True,\n","    'max_epochs' : 500,\n","    'batch_size': 32,\n","    'input-channels':3,\n","    'output-channels':2,\n","    'feature' : 1,\n","    'VGG_FACE_WEIGHTS':data_path + 'vgg_face_dag.pth',\n","    'vgg' : 'vgg-face',\n","    'lr': 2e-5,\n","    'weight': 0.2\n","    }\n","checkpoint_dir = model_path + 'VGG'\n","check_point_callback = pl.callbacks.ModelCheckpoint(\n","    monitor='valid_loss',  \n","    verbose=True, \n","    save_top_k = 2,  \n","    mode='min', \n","    dirpath= checkpoint_dir,  \n","    filename='{epoch}-{valid_f1:.4f}')\n","early_stopping = pl.callbacks.EarlyStopping(\n","    monitor='valid_loss',  \n","    patience = 10,  \n","    verbose=True,  \n","    mode='min',\n",")\n","model = classification(hparams)\n","model.to(torch_device)\n","model.train()\n","metric = Metrics(name = 'VGG_nose')\n","transform = models.VGG16_Weights.IMAGENET1K_V1.transforms\n","data_module = dataloader(data_path, transform, hparams['batch_size'], augment = hparams['augment'], gan = hparams['gan'])\n","trainer = pl.Trainer(val_check_interval=1.0, max_epochs = hparams['max_epochs'], default_root_dir= checkpoint_dir,  callbacks = [check_point_callback, metric, early_stopping])"],"metadata":{"id":"VDA2r7-jBLwF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, batch_size = 32, augment = True, gan = False\n","ckpt_path = model_path + 'best_checkpoints/best_nose'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"xLo0XUiMBTtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, batch_size = 32, augment = False, gan = False\n","ckpt_path = model_path + 'best_checkpoints/best_nose_weight=0.3_noaugment_78.9182.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"bKIaGwBiByuT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Mouth"],"metadata":{"id":"_l-Sf3zMDJ8S"}},{"cell_type":"code","source":["# VGG16 Model with augmentation for syndrome/no-syndrome classification pretrained over vgg-face\n","landmarks = 3\n","hparams = {\n","    'augment' : True,\n","    'max_epochs' : 500,\n","    'batch_size': 32,\n","    'input-channels':3,\n","    'output-channels':2,\n","    'feature' : 2,\n","    'VGG_FACE_WEIGHTS':data_path + 'vgg_face_dag.pth',\n","    'vgg' : 'vgg-face',\n","    'lr': 2e-5,\n","    'weight': 0.2\n","    }\n","checkpoint_dir = model_path + 'VGG'\n","check_point_callback = pl.callbacks.ModelCheckpoint(\n","    monitor='valid_loss',  \n","    verbose=True, \n","    save_top_k = 2,  \n","    mode='min', \n","    dirpath= checkpoint_dir,  \n","    filename='{epoch}-{valid_f1:.4f}')\n","early_stopping = pl.callbacks.EarlyStopping(\n","    monitor='valid_loss',  \n","    patience = 10,  \n","    verbose=True,  \n","    mode='min',\n",")\n","model = classification(hparams)\n","model.to(torch_device)\n","model.train()\n","metric = Metrics(name = 'VGG_mouth')\n","transform = models.VGG16_Weights.IMAGENET1K_V1.transforms\n","data_module = dataloader(data_path, transform, hparams['batch_size'], augment = hparams['augment'], gan = hparams['gan'])\n","trainer = pl.Trainer(val_check_interval=1.0, max_epochs = hparams['max_epochs'], default_root_dir= checkpoint_dir,  callbacks = [check_point_callback, metric, early_stopping])"],"metadata":{"id":"ATzUEsGzDOLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.2, batch_size = 32, augment = True, gan = False\n","ckpt_path = model_path + 'best_checkpoints/best_mouth_weights=0.2_70-9126.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"0zMCK5dODMKW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, batch_size = 32, augment = False, gan = False\n","ckpt_path = model_path + 'best_checkpoints/best_mouth_weights=0.3_noaugment.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"Op9AB7h-DY6P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Forehead - Ears and Chin"],"metadata":{"id":"PLidnrrvD39G"}},{"cell_type":"markdown","source":["#### Forehead and Ears"],"metadata":{"id":"ichZOVutEBtW"}},{"cell_type":"code","source":["# VGG16 Model with augmentation for syndrome/no-syndrome classification pretrained over vgg-face\n","landmarks = 2\n","hparams = {\n","    'augment' : True,\n","    'max_epochs' : 500,\n","    'batch_size': 32,\n","    'input-channels':3,\n","    'output-channels':2,\n","    'feature' : 0,\n","    'VGG_FACE_WEIGHTS':data_path + 'vgg_face_dag.pth',\n","    'vgg' : 'vgg-face',\n","    'lr': 2e-5,\n","    'weight': 0.2\n","    }\n","checkpoint_dir = model_path + 'VGG'\n","check_point_callback = pl.callbacks.ModelCheckpoint(\n","    monitor='valid_loss',  \n","    verbose=True, \n","    save_top_k = 2,  \n","    mode='min', \n","    dirpath= checkpoint_dir,  \n","    filename='{epoch}-{valid_f1:.4f}')\n","early_stopping = pl.callbacks.EarlyStopping(\n","    monitor='valid_loss',  \n","    patience = 10,  \n","    verbose=True,  \n","    mode='min',\n",")\n","model = classification(hparams)\n","model.to(torch_device)\n","model.train()\n","metric = Metrics(name = 'VGG_forehead')\n","transform = models.VGG16_Weights.IMAGENET1K_V1.transforms\n","data_module = dataloader(data_path, transform, hparams['batch_size'], augment = hparams['augment'], gan = hparams['gan'])\n","trainer = pl.Trainer(val_check_interval=1.0, max_epochs = hparams['max_epochs'], default_root_dir= checkpoint_dir,  callbacks = [check_point_callback, metric, early_stopping])"],"metadata":{"id":"XqZkpPs1D_Dv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, batch_size = 32, augment = True, gan = False\n","ckpt_path = model_path + 'best_checkpoints/best_forehead.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"U849fRuWEL9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.2, batch_size = 32, augment = True, gan = True\n","ckpt_path = model_path + 'best_checkpoints/best_forehead_weight=0.2_loss=0.314_gan.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"4YOXoHRQEnAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, batch_size = 32, augment = False, gan = False\n","ckpt_path = model_path + 'best_checkpoints/best_forehead_weight=0.3_loss_3.56_noaugmentation.ckpt'\n","trainer = pl.Trainer()\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"AMNWuxRkE9gk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Chin"],"metadata":{"id":"8CaLdIeaFo1M"}},{"cell_type":"code","source":["# VGG16 Model with augmentation for syndrome/no-syndrome classification pretrained over vgg-face\n","landmarks = 2\n","hparams = {\n","    'augment' : True,\n","    'max_epochs' : 500,\n","    'batch_size': 32,\n","    'input-channels':3,\n","    'output-channels':2,\n","    'feature' : 1,\n","    'VGG_FACE_WEIGHTS':data_path + 'vgg_face_dag.pth',\n","    'vgg' : 'vgg-face',\n","    'lr': 2e-5,\n","    'weight': 0.2\n","    }\n","checkpoint_dir = model_path + 'VGG'\n","check_point_callback = pl.callbacks.ModelCheckpoint(\n","    monitor='valid_loss',  \n","    verbose=True, \n","    save_top_k = 2,  \n","    mode='min', \n","    dirpath= checkpoint_dir,  \n","    filename='{epoch}-{valid_f1:.4f}')\n","early_stopping = pl.callbacks.EarlyStopping(\n","    monitor='valid_loss',  \n","    patience = 10,  \n","    verbose=True,  \n","    mode='min',\n",")\n","model = classification(hparams)\n","model.to(torch_device)\n","model.train()\n","metric = Metrics(name = 'VGG_chin')\n","transform = models.VGG16_Weights.IMAGENET1K_V1.transforms\n","data_module = dataloader(data_path, transform, hparams['batch_size'], augment = hparams['augment'], gan = hparams['gan'])\n","trainer = pl.Trainer(val_check_interval=1.0, max_epochs = hparams['max_epochs'], default_root_dir= checkpoint_dir,  callbacks = [check_point_callback, metric, early_stopping])\n"],"metadata":{"id":"-i8Qi4wAFrHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, batch_size = 32, augment = True, gan = False\n","ckpt_path = model_path + 'best_checkpoints/best_chin'\n","trainer = pl.Trainer()\n","data_module = dataloader(data_path, transform, hparams['batch_size'], augment = hparams['augment'], gan = hparams['gan'])\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"TnPNp5eCFugr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG weights = 0.3, batch_size = 32, augment = False, gan = False\n","ckpt_path = model_path + 'best_checkpoints/best_chin_weight=0.3_loss=0.35_noaugmented.ckpt'\n","trainer = pl.Trainer()\n","data_module = dataloader(data_path, transform, hparams['batch_size'], augment = hparams['augment'], gan = hparams['gan'])\n","trainer.test(model = model, ckpt_path = ckpt_path, datamodule = data_module)"],"metadata":{"id":"np4dqZh0GyCA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PA-GAN"],"metadata":{"id":"M6Y9OME5IFYk"}},{"cell_type":"code","source":["!pip install tensorflow-graphics-gpu --no-deps oyaml tensorflow-addons tf-slim"],"metadata":{"id":"X0ojaacAIICu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/uni/EAI/model/PA-GAN"],"metadata":{"id":"OIK21P8fIMk1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/uni/EAI/model/PA-GAN/test.py \\\n","--experiment_name PA-GAN_128 \\\n","--img_dir /content/drive/MyDrive/uni/EAI/model/PA-GAN/data/aligned/data \\\n","--test_label_path /content/drive/MyDrive/uni/EAI/model/PA-GAN/data/aligned/test_label.txt"],"metadata":{"id":"tGBP1n_RIPWq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python /content/drive/MyDrive/uni/EAI/model/PA-GAN/scripts/align.py \\\n","--img_dir /content/drive/MyDrive/uni/EAI/model/PA-GAN/data/images \\\n","--save_dir /content/drive/MyDrive/uni/EAI/model/PA-GAN/data/aligned \\\n","--standard_landmark_file /content/drive/MyDrive/uni/EAI/model/PA-GAN/data/standard_landmark_68pts.txt\\\n","--landmark_file /content/drive/MyDrive/uni/EAI/model/PA-GAN/data/landmarks.txt"],"metadata":{"id":"WmQbArU0IR6n"},"execution_count":null,"outputs":[]}]}